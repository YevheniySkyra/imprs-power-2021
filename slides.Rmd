---
title: "Power analyses and simulations in R"
author: "Andrew Jessop"
output: 
  revealjs::revealjs_presentation:
    theme: white
    highlight: kate
    center: true
    smart: true
    incremental: false
    transition: fade
    css: "support/style.css"
    self_contained: true
    reveal_options:
      slideNumber: false
      controls: false
      progress: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)

# Function to apply global formatting to plots
trek <- c("#F04540", "#0189FF", "#23CC8C", "#AA78FD", "#FF9109", "#95ACBD")
format_plot <- function(p) {
  p <- p + theme_bw()
  p <- p + theme(plot.title = element_text(lineheight=.8, size=14, hjust = 0.5, face="bold"),
               axis.title = element_text(size = 10, color="black", face="bold"),
               axis.text = element_text(size = 10, color="black"),
               panel.background = element_rect(fill = "white", colour = "black"),
               legend.key.size = unit(1,"line"),
               legend.title = element_text(face = "bold", size = 11),
               legend.text = element_text(size = 11),
               legend.position="bottom",
               panel.grid.major = element_blank(),
               panel.grid.minor = element_blank(),
               strip.text = element_text(size = 14, color="black", face="bold"),
               strip.background = element_rect(fill = "white", colour = "black"))
  return(p)
}

```

## Goal

- To run your own power analyses in R

- To understand why more data is better


## How

- Look at a range of different scenarios where power analysis is helpful

- Using the [pwr](https://www.rdocumentation.org/packages/pwr/versions/1.3-0), [simr](https://www.rdocumentation.org/packages/simr/versions/1.0.5), and [tidyverse](https://www.tidyverse.org/) packages in R

- It is possible to run full-scale power analyses with just a few lines of code

- For more complex use cases, you might need to build a bespoke simulation.

- We won't cover that today, but see [here](https://github.com/andrew-jessop/imprs-power-analysis-workshop.git)

## What is power?

- The probability of finding an effect statistically different from 0, if there is a true effect to be found

- Power analysis is often used to determine the number of subjects we need to test to have a reasonable chance (80%) of observing our hypothesised effects -- apriori power analysis

- It is also commonly used to calculate the probability of finding an effect in a sample already collected -- posthoc power analysis

## Scenario 1

>- You are designing a study investigating whether vocabulary size at 18 months correlates with concurrent lexical processing efficiency
>- The smallest relationship that you would consider theoretically interesting is a Pearson's r of 0.3 (medium effect; Cohen, 1992)
>- You will reject the null hypothesis when p < .05 (alpha level) in a two-sided test
>- **How many subjects (data points) do you need to recruit to have an 80% chance of observing this relationship?**

## Scenario 2

>- A new meta-analysis has been published suggesting that the average correlation between vocabulary and reaction times is 0.2 (a small to medium effect; Cohen, 1992)
>- On this basis, you now want to ensure that you have enough data to observe a correlation of at least 0.2
>- You will reject the null hypothesis when p < .05 (alpha level) in a two-sided test
>- **How many subjects do you need to have an 80% chance of observing this relationship?**


##

```{r, fig.width=10, fig.height=5}


plot_text <- paste0("The null hypothesis states\nthat the true effect size is 0\n\n",
"As the magnitude of an\nobserved effect size increases,\nits likelihood under the\nnull hypothesis decreases\n\n",
"Instead, there is a reduced risk\n of making a Type I error")

p <- ggplot(data = data.frame(x = c(-0.5, 1)), aes(x))
p <- p + geom_vline(xintercept = 0, colour = "grey", linetype=2)
p <- p + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 0.1),
                       colour=trek[2], size=1.5)
p <- p + ylab("Probability Density") + xlab("Pearson's R")
p <- p + scale_y_continuous(breaks = NULL)
p <- p + scale_x_continuous(breaks = seq(-0.5, 1, 0.1))
p <- p + coord_cartesian(ylim = c(0, 5))
p <- p + annotate(geom="text",label="Null", y = 4.25, x = 0,
                  fontface="bold", size = 6, colour=trek[2])
p <- p + annotate(geom="text",label=plot_text,
                  y = 2.75, x = 0.6, fontface="bold", size = 6)
p <- format_plot(p)
p

```

##

```{r, fig.width=10, fig.height=5}

plot_text <- paste0("We control the Type I error rate\n",
                    "by setting a limit at which we\n",
                    "reject the null hypothesis\n",
                    "and accept the alternative\n",
                    "hypothesis\n\n",
                    "This is called the alpha level\n",
                    "and it is typically set at p < .05\n\n")
p <- ggplot(data = data.frame(x = c(-0.5, 1)), aes(x))
p <- p + geom_vline(xintercept = 0, colour = "grey", linetype=2)
p <- p + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 0.1),
                       colour=trek[2], size=1.5)
p <- p + geom_vline(xintercept = 0.2, colour = "red", linetype=2)
p <- p + ylab("Probability Density") + xlab("Pearson's R")
p <- p + scale_y_continuous(breaks = NULL)
p <- p + scale_x_continuous(breaks = seq(-0.5, 1, 0.1))
p <- p + coord_cartesian(ylim = c(0, 5))
p <- p + annotate(geom="text",label="Null", y = 4.25, x = 0,
                  fontface="bold", size = 6, colour=trek[2])
p <- p + annotate(geom="text",label=plot_text,
                  y = 2.25, x = 0.6, fontface="bold", size = 6)
p <- p + annotate(geom="text", label="Critical effect size",
                  y = 5, x = 0.4, fontface="bold", size = 6,
                  colour=trek[1])
p <- format_plot(p)
p

```

##

```{r, fig.width=10, fig.height=5}

p <- ggplot(data = data.frame(x = c(-0.5, 1)), aes(x))
p <- p + geom_vline(xintercept = 0, colour = "grey", linetype=2)
p <- p + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 0.1),
                       colour=trek[2], size=1.5)
p <- p + geom_vline(xintercept = 0.2, colour = "red", linetype=2)
p <- p + geom_vline(xintercept = 0.3, colour = trek[3], linetype=2)
p <- p + ylab("Probability Density") + xlab("Pearson's R")
p <- p + scale_y_continuous(breaks = NULL)
p <- p + scale_x_continuous(breaks = seq(-0.5, 1, 0.1))
p <- p + coord_cartesian(ylim = c(0, 5))
p <- p + annotate(geom="text",label="Null", y = 4.25, x = 0,
                  fontface="bold", size = 6, colour=trek[2])
p <- p + annotate(geom="text",label="Observed", y = 4.25, x = 0.43,
                  fontface="bold", size = 6, colour=trek[3])
p <- format_plot(p)
p
```


##

```{r, fig.width=10, fig.height=5}

plot_text <- paste0("With more data, the null distribution\n",
                    "becomes more concentrated and \n",
                    "even small effects have a low probability\n\n",
                    "This means the critical value (p < .05)\n",
                    "where we reject the null hypothesis\n",
                    "is also smaller\n\n",
                    "The position of the critical value\n",
                    "Is determined by the sample size")
p <- ggplot(data = data.frame(x = c(-0.5, 1)), aes(x))
p <- p + geom_vline(xintercept = 0, colour = "grey", linetype=2)
p <- p + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 0.05),
                       colour=trek[2], size=1.5)
p <- p + geom_vline(xintercept = 0.1, colour = "red", linetype=2)
p <- p + ylab("Probability Density") + xlab("Pearson's R")
p <- p + scale_y_continuous(breaks = NULL)
p <- p + scale_x_continuous(breaks = seq(-0.5, 1, 0.1))
p <- p + coord_cartesian(ylim = c(0, 8.5))
p <- p + annotate(geom="text",label="Null", y = 8.25, x = 0,
                  fontface="bold", size = 6, colour=trek[2])
p <- p + annotate(geom="text",label=plot_text,
                  y = 4, x = 0.55, fontface="bold", size = 6)
p <- p + annotate(geom="text", label="Critical effect size",
                  y = 8.5, x = 0.3, fontface="bold", size = 6,
                  colour=trek[1])
p <- format_plot(p)
p

```

##

```{r, fig.width=10, fig.height=5}

p <- ggplot(data = data.frame(x = c(-0.5, 1)), aes(x))
p <- p + geom_vline(xintercept = 0, colour = "grey", linetype=2)
p <- p + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 0.05),
                       colour=trek[2], size=1.5)
p <- p + geom_vline(xintercept = 0.1, colour = "red", linetype=2)
p <- p + ylab("Probability Density") + xlab("Pearson's R")
p <- p + scale_y_continuous(breaks = NULL)
p <- p + scale_x_continuous(breaks = seq(-0.5, 1, 0.1))
p <- p + coord_cartesian(ylim = c(0, 8.5))
p <- p + annotate(geom="text",label="Null", y = 8.25, x = 0,
                  fontface="bold", size = 6, colour=trek[2])
p <- format_plot(p)
p

```

## Apriori power analysis

- Power analyses allow us design our study so that we have enough data so that our critical value is smaller than the effect we expect to observe

- With enough data, you can plan a study to detect even very small relationships or effects that you are interested in


## Can you overpower a study?

- It is possible to continously collect data until you reach significance (*p* < .05)
- This is a form of p-hacking (or data-dredging)
- It is a problem when we use NHST as the basis of scientific discovery
- Not a big problem when we focus our attention on the effect size, as they are more difficult to hack
- A reliable correlation where r = 0.01 (and *p* < .05) may not be very interesting

## Can you overpower a study?

- No! No! No!
- This is a weakness of the NHST approach
- What matters is how you interpret the effects
- More data offers more reliable effect size estimates with narrower CIs or posterior distributions
- More data allows you to perform more sophisticated analyses or use more complex model specifications


## Scenario 3

>- You have conducted a study that did not find a significant relationship (between vocabulary and reaction times)
>- One reviewer is concerned about this null finding and suggests that your experiment is underpowered
>- You collected 200 participants and observed a Pearson's r of 0.13
>- You decided that you would reject the null hypothesis when p < .05 (two-sided)
>- The smallest effect you would consider theoretically interesting is 0.2
>- **Is this study underpowered?**


## Observed power is flawed

- See Hoenig and Heisey (2001)

- Observed p-value also determines the observed power (1:1 function)

- Observed effect $\neq$ true effect 

- True power depends on an unknown true effect size

- Confidence intervals, an equivalence test, or a Bayes factor would be more appropriate for exploring the data at hand


## Calculate power based on...

1) Previous observations - meta-analyses or similar studies with *large* sample sizes 

2) The smallest effect size that would be theoretically interesting -- requires some thought and is probably the most important step of a power analysis



## Scenario 4

>- You are planning a new and innovative individual differences study
>- There is very little previous evidence, but you have decided that a Pearson's r of 0.25 is the smallest relationship you would consider theoretically interesting
>- Since this is a new area of enquiry, you have decided that an alpha level of .05 is not small enough to reject the null hypothesis


## What is the p-value?

- The probability of obtaining test results at least as extreme as those observed, assuming that the null hypothesis is correct

- We reject the null once *p* falls below the *alpha level*, which is what we consider an acceptable Type I error rate

- Type I error = rejecting the null hypothesis when it is true (no effect exists but you think there is one)

- A false positive


## Modern approaches

- *p* < .05 is convenient but arbitrary
- *p* < .005 for new discoveries (Benjamin et al., 2018)
- Justify specific alpha levels (Lakens et al., 2018)
- Abandon significance and treat *p* as continuous (McShane et al., 2019)
- (Burn all the frequentist text books and go Bayesian)


## Scenario 4

- You are planning a new and innovative individual differences study
- There is very little previous evidence, but you have decided that a Pearson's r of 0.25 is the smallest relationship you would consider theoretically interesting
- Since this is a new area of enquiry, you have decided that an alpha level of .05 is not small enough to reject the null hypothesis
- You have decided to use an alpha level of .001 (1/1000)
- **How many subjects do you need to collect to reach 80% power**


## Scenario 5

>- Since this is a new area of research, you want make sure that you're not making a Type II error
>- You have decided you want a 95% chance observing the hypothesised relationship (r = 0.25), if it exists


## What is power?

- Type II error = failing to reject the null when it is false (you think there is no effect when it actually exists)

- False negative

- Beta level = the probability of a Type II error
 
 
## What is power?

- Power = 1 - $\beta$

- The probability of avoiding a Type II error

- The power to detect a *true* effect when it exists

- 80% is considered the minimum acceptable level


## Scenario 5

>- Since this is new area of research, you want make sure that you're not making a Type II error
>- You have decided you want a 95% chance observing the hypothesised relationship (r = 0.25), if it exists
>- **How many subjects do you need for 95% power**


## Scenario 6

>- You have realised that your hypothesis is directional - you are predicting a positive correlation
>- You switch to a one-tailed test instead of a two-tailed test
>- **How many subjects do you need for 95% power**


## Two-tailed

```{r, fig.width=10, fig.height=5}

p <- ggplot(data = data.frame(x = c(-0.5, 1)), aes(x))
p <- p + geom_vline(xintercept = 0, colour = "grey", linetype=2)
p <- p + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 0.1),
                       colour=trek[2], size=1.5)
p <- p + geom_vline(xintercept = 0.225, colour = "red", linetype=2)
p <- p + geom_vline(xintercept = -0.225, colour = "red", linetype=2)
p <- p + ylab("Probability Density") + xlab("Pearson's R")
p <- p + scale_y_continuous(breaks = NULL)
p <- p + scale_x_continuous(breaks = seq(-0.5, 1, 0.1))
p <- p + coord_cartesian(ylim = c(0, 5))
p <- p + annotate(geom="text",label="Null", y = 4.25, x = 0,
                  fontface="bold", size = 6, colour=trek[2])
p <- format_plot(p)
p

```

## One-tailed

```{r, fig.width=10, fig.height=5}

p <- ggplot(data = data.frame(x = c(-0.5, 1)), aes(x))
p <- p + geom_vline(xintercept = 0, colour = "grey", linetype=2)
p <- p + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 0.1),
                       colour=trek[2], size=1.5)
p <- p + geom_vline(xintercept = 0.175, colour = "red", linetype=2)
p <- p + ylab("Probability Density") + xlab("Pearson's R")
p <- p + scale_y_continuous(breaks = NULL)
p <- p + scale_x_continuous(breaks = seq(-0.5, 1, 0.1))
p <- p + coord_cartesian(ylim = c(0, 5))
p <- p + annotate(geom="text",label="Null", y = 4.25, x = 0,
                  fontface="bold", size = 6, colour=trek[2])
p <- format_plot(p)
p

```


## Four critical parameters

1) The effect size of interest

2) The sample size

3) The power level

4) The alpha level


## Psychology is underpowered

- Cohen (1962): "...nonrational bases for setting sample size must often result in investigations being undertaken which have little chance of success despite the actual falsity of the null hypothesis"

- Things have not improved much since then (Button et al., 2013; Smaldino & McElreath, 2016)

- Less than 40% power seems typical of our field

## SIMULATION TIME!

## Conclusions

- The typical sample sizes used in psycholinguistic research are not enough

- Especially true now that we are using more mixed-effects models, which carry a power cost (Matuschek et al., 2017)

- We should aim to have the largest sample sizes


## Take home message

- Calculate your power levels **before** data collection

- Focus on **effect sizes** not p-values

- **More data is always better**
